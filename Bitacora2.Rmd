# Bitacora 2

```{r}
library(readr)
library(dplyr)
library(tidyverse)
library(formattable)
library(kableExtra)
library(xtable)
library(ggplot2)
library(hrbrthemes)
library(viridis)
```


## Fichas bibliográficas nuevas


- \textbf{Título:} Modelado de parejas aleatorias usando cópulas

- \textbf{Autor(es):} Gabriel Escarela & Angélica Hernández

- \textbf{Año:} 2009

- \textbf{Nombre del tema:} Modelos de cópulas

- \textbf{Forma de organizarlo:}

  -\textbf{Cronológico:} 2009
  
  -\textbf{Metodológico:} Modelos de cópulas
  
  -\textbf{Temático:}  Cópulas bivariadas
  
  -\textbf{Teoría:} Teoría de cópulas

- \textbf{Resumen en una oración:} Definición formal de las cópulas bivariadas junto con ejemplos de su utilidad

- \textbf{Argumento central:} Teoría de cópulas y su construcción teórica

- \textbf{Problemas con el tema:} Solo funcionan cuando se tienen 2 variables. 

- \textbf{Resumen en un párrafo:} Las cópulas bivariadas son funciones que intentan correlacionar dos distribuciones univariadas por lo que estos modelos ayudan a obtener una distribución conjunta a partir de varias funciones de distribución asociadas a variables aleatorias con una cierta relación entre sí. Es decir, con este método se construye una distribución multivariada a partir de las distribuciones univariadas de las variables respectivas. Este tipo de modelo también resulta en una forma de estructurar la dependencia de estas parejas de variables aleatorias en distribuciones conjuntas. Es por esto que son tan populares puesto que tienen una gran flexibilidad para encontrar distribuciones conjuntas a partir de cualquier pareja aleatoria, lo que es usual tener en muchas disciplinas.

\noindent\rule{14cm}{0.4pt}

- \textbf{Título:} An Introduction to Statistical
Learning with Applications in R - Chapter 4: Classification

- \textbf{Autor(es):} Gareth James, Daniela Witten, Trevor Hastie & Robert Tibshirani

- \textbf{Año:} 2021

- \textbf{Nombre del tema:} Métodos de clasificación

- \textbf{Forma de organizarlo:}

  -\textbf{Cronológico:} 2021
  
  -\textbf{Metodológico:} Métodos de clasificación
  
  -\textbf{Temático:}  Regresión logística para clasificación
  
  -\textbf{Teoría:} Realizar modelos de clasificación 

- \textbf{Resumen en una oración:} Implementar métodos de clasificación y sus pruebas para determinar la calidad del modelo.

- \textbf{Argumento central:} Métodos de clasificación y sus pruebas con ejemplos programados en R.

- \textbf{Problemas con el tema:} No es un problema como tal, pero se menciona que hay que tener cuidado pues hay varios métodos de clasificación y tal vez el escogido no sea el más conveniente. 

- \textbf{Resumen en un párrafo:} A diferencia de los métodos de regresión, se teoriza un nuevo tipo de regresión llamada regresión logística, la cual sirve para clasificar de manera binaria una variable. Entonces en vez de entrenar un modelo para determinar si hay una correlación entre la variable dependiente y las covariables, se entrena para clasificar en alguna de dos categorías a la variable dependiente de acuerdo a sus covariables. También se mencionan diagnósticos de los modelos de clasificación que ayudan a determinar si un modelo es mejor que otro o está mejor ajustado como por ejemplo el concepto de especificidad y sensibilidad los cuales miden la tasa de falsos positivos y los falsos negativos respectivamente. Y como de acuerdo al tipo de problema para el cual se está realizando el modelo, se debe considerar ajustes del modelo para aumentar estas medidas. Otro diágnostico bastante utilizado es el de la curva ROC (Receiver Operation Curve), la cual sirve para graficar la tasa de falsos positivos con la sensibilidad del modelo. 

\noindent\rule{14cm}{0.4pt}

- \textbf{Título:} La regresión logística

- \textbf{Autor(es):} Horacio Chitarroni

- \textbf{Año:} 2002

- \textbf{Nombre del tema:} Regresión logística

- \textbf{Forma de organizarlo:}

  -\textbf{Cronológico:} 2002
  
  -\textbf{Metodológico:} Métodos de clasificación
  
  -\textbf{Temático:}  Regresión logística para clasificación
  
  -\textbf{Teoría:} Realizar modelos de clasificación 

- \textbf{Resumen en una oración:} Implementar métodos de clasificación con ejemplos e interpretación de resultados

- \textbf{Argumento central:} Método de regresión logística y su implementación 

- \textbf{Problemas con el tema:} El autor no menciona problemas.

- \textbf{Resumen en un párrafo:} El modelo de regresión logística es un instrumento de análisis multivariado y dependiendo del enfoque se puede utilizar para realizar predicciones o inferencia. Se menciona que es muy útil cuando la variable dependiente es de carácter dicotómico (binario). Asimismo, se aclara que cuando las covariables son categóricas estas deberían de recibir una transformación y convertirlas en variables "dummy", es decir, variables simuladas. Este tipo de modelo se puede utilizar de manera predictiva, el cual no solo determina la probabilidad de la variable dependiente sino el peso que tienen las covariables para realizar la predicción, lo cual también ayuda a nivel interpretativo pues se pueden determinar que variables son más significativas que otras para el modelo. 
\noindent\rule{14cm}{0.4pt}

\newpage

## Ordenamiento de literatura

La literatura se va a ordenar de manera metodológica por lo que se tienen dos grupos, la literatura ligada a los métodos de clasificación, y la literatura enfocada en cópulas. 

```{r Ordenamiento Literatura Regresion}

r1 <- c("Metodológico", "Regresión logística", "Métodos de clasificación","An Introduction to Statistical
Learning","2021", "James et al.")

r2 <- c("Metodológico", "Regresión logística", "Regresión logística","La regresión logística","2002", "Horacio Chitarroni")

data.liter <- data.frame(rbind(r1,r2))

rownames(data.liter) <- c()

data.liter %>% kbl(caption="Clasificación Binaria",
      format="latex",
      row.names = NA,
      col.names = c("Tipo de grupo","Nombre del grupo","Nombre del tema","Título","Año","Autor(es)"), align = 'c')%>%
      kable_styling(latex_options = c("HOLD_position"),full_width = T)

```

```{r Ordenamiento Literatura Copulas}

r1 <- c("Metodológico", "Cópulas", "Grouped t-Copulas ","The Grouped t-copula with an Application to Credit Risk","2003", "Daul et al.")

r2 <- c("Metodológico", "Cópulas", "Vine Copulas","Application of Vine Copulas to Credit Portfolio
Risk Modeling","2016", "Geidosch & Fischer")

r3 <- c("Metodológico", "Cópulas", "Cópulas bivariadas","Modelado de parejas aleatorias usando cópulas","2009", "Escarela & Hernández") 

data.liter <- data.frame(rbind(r1,r2,r3))

rownames(data.liter) <- c()

data.liter %>% kbl(caption="Cópulas",
      format="latex",
      row.names = NA,
      col.names = c("Tipo de grupo","Nombre del grupo","Nombre del tema","Título","Año","Autor(es)"), align = 'c')%>%
      kable_styling(latex_options = c("HOLD_position"),full_width = T)

```

\newpage

## Análisis estadístico

Dado que la mayor\'ia de variables son categ\'oricas, primero se realizar\'a un proceso de depuración de la base con el fin de reducir el n\'umero de variables estudiados.  Para ello, primeramente se reducirán la cantidad de categorías que existen en cada uno de las variables combinando categorías que compartan carácterísticas o presenten muy pocas observaciones. Por ejemplo, en el caso de la variable del Próposito del Crédito, hay 11 categorías diferentes pero se simplificó de tal manera que solo hubiera 4 categorías. La primera son los préstamos relacionados a la compra de un Automóvil, ya sea nuevo o de segunda mano. La segunda a los préstamos realizados al Hogar, ya sea para compra de muebles o remodelaciones. Una tercera catogoría relacionada a créditos Empresariales y una última categoría que incluyera todos los préstamos cuya razón de solicitud no entre en las categorías anteriores. 


```{r Homologacion}

library(readr)
library(dplyr)
library(tidyverse)
library(formattable)
library(kableExtra)
library(xtable)
library(ggplot2)
library(hrbrthemes)
library(viridis)

data <- read_csv("german.csv")
attach(data)


data <- data %>% mutate(Account_Balance = case_when(Account_Balance == 1 ~ 1, 
                           Account_Balance == 2 ~ 2,
                           Account_Balance %in% c(3,4) ~ 3 ))

data <- data %>% mutate(Payment_Status_of_Previous_Credit = case_when(
  Payment_Status_of_Previous_Credit %in% c(0,1) ~ 1,
  Payment_Status_of_Previous_Credit %in% c(2,4) ~ 2,
  Payment_Status_of_Previous_Credit == 3 ~ 3))

data <- data %>%  mutate(Value_Savings_Stocks = case_when(
  Value_Savings_Stocks == 1 ~ 1, 
  Value_Savings_Stocks == 2 ~ 2,
  Value_Savings_Stocks %in% c(3,4) ~ 3,
  Value_Savings_Stocks == 5 ~ 4
))


data <- data %>% mutate(Length_of_current_employment = case_when(
  Length_of_current_employment %in% c(1,2) ~ 1, 
  Length_of_current_employment == 3 ~ 2, 
  Length_of_current_employment == 4 ~ 3, 
  Length_of_current_employment == 5 ~ 4
))

data <- data %>% mutate(Sex_Marital_Status = case_when(
  Sex_Marital_Status %in% c(1,2) ~ 1, 
  Sex_Marital_Status == 3 ~ 2, 
  Sex_Marital_Status == 4 ~ 3
))

data <- data %>%  mutate(No_of_Credits_at_this_Bank = case_when(
  No_of_Credits_at_this_Bank == 1 ~ 1, 
  No_of_Credits_at_this_Bank %in% c(2,3,4) ~ 2
))

data <- data %>% mutate(Guarantors = case_when(
  Guarantors == 1 ~ 1, 
  Guarantors == 2 ~ 2
))

data <- data %>% mutate(Concurrent_Credits = case_when(
  Concurrent_Credits %in% c(1,2) ~ 1, 
  Concurrent_Credits == 3 ~ 2
))

data <- data %>%  mutate(Purpose = case_when(
  Purpose %in% c(1,2) ~ 1, 
  Purpose %in% c(3,4,5,6) ~ 2, 
  Purpose == 10 ~ 3,
  Purpose %in% c(7,8,9,0) ~ 4
))



```


Una vez hecha estas modificaciones, se puede extraer información interesante como la tabla de frecuencias según el Próposito del préstamo. 

```{r  Tabla 1}

tabla1 <- data %>% group_by(Purpose) %>% summarize(n =n())

tabla1 %>% kbl(caption="Distribución de la variable Próposito del Crédito",
      format="latex",
      col.names = c("Propósito","Cantidad de observaciones"), align = 'c')%>% 
  footnote(general = "Elaboración propia con datos extraídos de Kaggle") %>% kable_styling(latex_options = "HOLD_position")
```


La tabla anterior revela que la mayoría de préstamos solicitados son con asuntos relacionadas al hogar, mientras que hay una porción muy bajo de préstamos destinados al área empresarial. Visto de manera gráfica: 

```{r Graph6, fig.cap="Distribución de la variable Próposito del crédito"}

graph6 <- tabla1 %>% ggplot(aes(x = Purpose, y = n)) +
  geom_bar(stat="identity", alpha=.6, width=.3, color = "black", fill = "antiquewhite")+
  #coord_flip()+
  theme_test()+
  labs(x = "", y = "", caption = "Elaboración propia con datos extraídos de Kaggle")+
  scale_x_discrete(limits = c("Auto", "Hogar", "Empresa", "Otro"))
graph6

```


Asimismo, en el siguiente gráfico se muestra la relación que existe entre el monto del crédito según su razón, donde cabe destacar que aunque los motivos empresariales es la razón menos frecuente en la base de datos, el crédito solicitado de mayor monto tiene como razón dicho motivo. Mediante este análisis fue que se descartó la posibilidad de combinar la prósito "Empresa" con alguna otra, pues es de esperarse que haya información importante que pueda revelar. 


```{r Graph5, fig.cap="Distribución del Monto del Crédito según su próposito"}
graph5 <- data %>% ggplot(aes(x=Purpose, y = Credit_Amount)) + 
  geom_point(fill = "antiquewhite", color = "black", shape =  23)+
  theme_light()+
  labs(x = "", y = "", caption = "Elaboración propia con datos extraídos de Kaggle")+
  scale_x_discrete(limits = c("Auto", "Hogar", "Empresa", "Otro"))
graph5
```


Haciendo una análisis similar para las edades, se llega a la conclusión de que existe  una tendencia mayor en las personas cuyas edades estén en el rango de 20 a 40 años a solicitar préstamos como lo muestra la siguiente tabla:


```{r Tabla 2}
tabla2 <- data %>%  group_by(Age = cut(Age_years, breaks = c(10,20,30,40,50,60,70,80))) %>% count(Age) %>% na.omit(tabla1) 

tabla2 %>% kbl(caption="Distribución de las edades",
      format="latex",
      col.names = c("Rango de edad","Cantidad de observaciones"), align = 'c') %>% 
  footnote(general = "Elaboración propia con datos extraídos de Kaggle")%>% kable_styling(latex_options = "HOLD_position")

```

De manera gráfica, la densidad de la variable edad viene dada por el siguiente gráfico donde se nota una asimetría hacia a la derecho que deja en evidencia lo que se habló anteriormente donde hay una tendencia mucho mayor en las personas entre los 20 y los 40 años de solictar préstamos. 

```{r Graph3, fig.cap="Histograma de la variable Edad"}

graph3 <- data %>% ggplot(aes(x = Age_years))+
  #geom_histogram(fill = "antiquewhite", color = "black")+
  geom_density()+
  theme_light()+
  labs(x = "Edad", y = "", caption = "Elaboración propia con datos extraídos de Kaggle")
graph3

```


Otra variable que es de esperarse que sea de importancia es el Balance Actual que el solicitante tiene. Esta variable es categórica y toma el valor de 0 en caso de que el solicitante no tenga ninguna cuenta abierta con el Banco, el valor 1 en caso de que  sí tenga una cuenta con el banco pero no tenga saldo o balance, y por último, toma el valor de 3 en caso de que sí tenga balance. La distribución de frecuencias viene dada por:


```{r  Tabla 4}
tabla4 <- data %>% group_by(Account_Balance) %>% summarize(n =n())

tabla4 %>% kbl(caption="Distribución de la variable Balance Actual",
      format="latex",
      col.names = c("Balance Actual","Cantidad de observaciones"), align = 'c') %>% 
  footnote(general = "Elaboración propia con datos extraídos de Kaggle")%>% kable_styling(latex_options = "HOLD_position")


```


La mayoría de variables que se encuentran en la base son de carácter categórico, sin embargo, el siguiente cuadro muestra información sobre las variables de carácter continuo: 


```{r  Tabla 3}
tabla3.1 <- data %>% summarize(
  Min = min(Credit_Amount),
  Q1 = quantile(Credit_Amount, 0.25),
  Mediana = median(Credit_Amount),
  Q3 = quantile(Credit_Amount,0.75),
  Max = max(Credit_Amount)
) 

tabla3.2 <- data %>% summarize(
  Min = min(Duration_of_Credit_monthly),
  Q1 = quantile(Duration_of_Credit_monthly, 0.25),
  Mediana = median(Duration_of_Credit_monthly),
  Q3 = quantile(Duration_of_Credit_monthly,0.75),
  Max = max(Duration_of_Credit_monthly)
) 


tabla3.3 <- data %>% summarize(
  Min = min(Age_years),
  Q1 = quantile(Age_years, 0.25),
  Mediana = median(Age_years),
  Q3 = quantile(Age_years,0.75),
  Max = max(Age_years)
) 

tabla3 <- rbind(tabla3.1, tabla3.2, tabla3.3)
rownames(tabla3) <- c("Monto del crédito", "Duración en meses del crédito", "Edad")

tabla3 %>% kbl(caption="Resumen de 5 números",
      format="latex", align = 'c') %>% 
  footnote(general = "Elaboración propia con datos extraídos de Kaggle")%>% kable_styling(latex_options = "HOLD_position")

```


Dado a que eventualmente la idea es realizar un modelo de Cópulas entre los resultados del proceso de clasificación de los solicitantes con la variable Monto del Crédito, sería importante describir de manera exhaustiva esta variable. A continuación un histograma que muestrala distribución de los montos: 

```{r Graph2, fig.cap= "Histograma de la variable Monto del Crédito"}
graph2 <- data %>% ggplot(aes(x = Credit_Amount))+
  geom_histogram(fill = "antiquewhite", color = "black")+
  theme_light()+
  labs(x = "Monto", y = "", caption = "Elaboración propia con datos extraídos de Kaggle")
graph2

```


El histograma muestra una fuerte asimetría hacia su derecho indicando la tendencia en la solicitud de crédito de montos bajos. Esto en efecto se puede ver el siguiente diagrama de caja y bigotes que también busca mostrar de manera más detallada la distribución intercuantílica de los montos:


```{r Graph1, fig.cap="Diagrama de Caja y Bigotes de la variable Monto del Crédito"}


data.graph1 <- data.frame(
  name = c(rep("Distribución",1000)),
  value = Credit_Amount
)

graph1 <- data.graph1 %>% ggplot(aes(x=name,y=Credit_Amount)) +
  geom_boxplot(width = 0.3, fill = "antiquewhite") +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  # theme_ipsum() +
  theme_bw()+
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  labs(x = "", y = "Monto", caption = "Elaboración propia con datos extraídos de Kaggle")

graph1

```


El gráfico es congruente con lo visto en el histograma y queda aún más en evidencia lo dicho anteriormente pues alrededor del 50$\%$ de los créditos solicitados fueron por montos menores a los 2500 DM que si comparamos este momento con el monto máximo solicitado de 18424 DM, se puede notar una gran diferencia. 

```{r  Tabla 5}
tabla5 <- data %>%  group_by(Foreign_Worker) %>%  summarize(n = n())

```

```{r Graph4}

graph4 <- data %>% ggplot(aes(x = Duration_of_Credit_monthly))+
  geom_density()+
  #geom_histogram(fill = "antiquewhite", color = "black")+
  theme_test()+
  labs(x = "Duración en meses", y = "", title = "Gráfico 2.4:  Histograma de la variable Duración del crédito", caption = "Elaboración propia con datos extraídos de Kaggle")


```

Dada la cantidad de variables categóricas con las que cuenta la base, se realizarán pruebas de tal manera que se puedan distinguir las variables de mayor relevancia y, a partir de las mismas, descartar las menos relevantes. Para ello se utilizará el la prueba de independencia Chi-Cuadrado. Se busca que los $p-values$ sean cercanos a cero con tal de afirmar que las variables son estadísticamente significantes en el estudio.  

```{r t test}

# Todas estas variables parecen ser significativas

# (t.amount <- t.test(table(Creditability, Credit_Amount))[3])
# (t.age <- t.test(table(Creditability, Age_years))[3])
# (t.duration <- t.test(Creditability, Duration_of_Credit_monthly)[3])

```


```{r Chi-Square test}

data.test <- data %>% dplyr::select(Account_Balance, Payment_Status_of_Previous_Credit, Purpose, Value_Savings_Stocks, Length_of_current_employment, Instalment_per_cent, Sex_Marital_Status, Duration_in_Current_address, Type_of_apartment,  Most_valuable_available_asset, No_of_Credits_at_this_Bank, Guarantors, Occupation, Concurrent_Credits, No_of_dependents,Telephone,Foreign_Worker)


dim <- dim(data.test)[2]
chi <- c()

for (i in 1:dim) {
  chi[i] <- chisq.test(table(Creditability, as.numeric(unlist(data.test[,i])) ))[3]
}

chi <- as.numeric(unlist(chi))

data.final <- data.test %>% dplyr::select(which(chi < 0.001)) %>% cbind(Credit_Amount, Age_years, Duration_of_Credit_monthly) %>%  relocate(Credit_Amount, Age_years, Duration_of_Credit_monthly)

```


```{r}
# (chi.account.balance <-chisq.test(table(Creditability, Account_Balance)))[3]
# (chi.payment <-chisq.test(table(Creditability, Payment_Status_of_Previous_Credit)))[3]
# (chi.purpose <-chisq.test(table(Creditability, Purpose)))[3]
# (chi.savings <- chisq.test(table(Creditability, Value_Savings_Stocks)))[3]
# (chi.length.employ <- chisq.test(table(Creditability, Length_of_current_employment)))[3]
# (chi.installment <- chisq.test(table(Creditability, Instalment_per_cent)))[3]
# (chi.sex <-chisq.test(table(Creditability, Sex_Marital_Status)))[3]
# (chi.address <-chisq.test(table(Creditability, Duration_in_Current_address)))[3]
# (chi.apartment <-chisq.test(table(Creditability, Type_of_apartment)))[3]
# (chi.asset <-chisq.test(table(Creditability, Most_valuable_available_asset)))[3]
# (chi.credits <-chisq.test(table(Creditability, No_of_Credits_at_this_Bank)))[3]
# (chi.guarantor <-chisq.test(table(Creditability, Guarantors)))[3]
# (chi.occupation <-chisq.test(table(Creditability, Occupation)))[3]
# (chi.concurrent <-chisq.test(table(Creditability, Concurrent_Credits)))[3]
# (chi.purpose <-chisq.test(table(Creditability, Purpose)))[3]
# (chi.dependents <-chisq.test(table(Creditability, No_of_dependents)))[3]
# (chi.telephone <-chisq.test(table(Creditability, Telephone)))[3]
# (chi.foreign <-chisq.test(table(Creditability, Foreign_Worker)))[3]
```



Estas pruebas mostraron que las variables más significativas son: Acount Balance, Payment Status, Savings/Stock Value, Length of Current Employment, Type Apartment, Most Valuable Asset, Concurrent Credits. A partir de estas variables se desarrolla un modelo de regresión logística en el toma un 60$\%$ para entrenamiento y un 40 $\%$ para testing. 



```{r Logistic Regression}
library(pROC)
library(caret)
library(verification)
library(ROCit)
library(plotROC)

indexes = sample(1:nrow(data), size=0.6*nrow(data))
train <- data[indexes,]
test <- data[-indexes,]

Logistic.1 <- glm(Creditability ~ Account_Balance + Payment_Status_of_Previous_Credit + Value_Savings_Stocks + Length_of_current_employment + Most_valuable_available_asset + Type_of_apartment + Concurrent_Credits + Duration_of_Credit_monthly + Credit_Amount + Age_years)

#summary(Logistic.1)

logit_P_real <- predict(Logistic.1, newdata = test, type = 'response')
logit_P <- ifelse(logit_P_real > 0.5, 1, 0)
actual <- as.numeric(unlist(test[,1]))
df <- data.frame(logit_P, actual)

CM1 <- confusionMatrix(as.factor(logit_P), as.factor(actual), mode = "everything", positive = "1")
```


Para determinar si al solicitante se le dará el crédito, el umbral será del 0.5, por lo que si el resultado de la regresión es mayor a 0.5, se le da el crédito y en caso contrario no. El resultado de la regresión arroja una curva ROC como se muestra a continuación:

```{r Logistic Regression ROC, fig.cap="Curva ROC"}
# roc_score <- roc(actual, logit_P, plot = TRUE)

# plot(rocit(score = logit_P, class = actual))

rocplot <- ggplot(df, aes(m = logit_P, d = actual)) + 
  geom_roc(n.cuts = 20, labels = FALSE) + 
  geom_abline(linetype = "dashed")+
  theme_bw() + 
  labs(x = "1-Especificidad", y = "Sensibilidad", caption = "Elaboración propia con datos extraídos de Kaggle")

rocplot

```



```{r Logistic Regression 2}

Logistic.2 <- glm(Creditability ~ Account_Balance + Payment_Status_of_Previous_Credit + Value_Savings_Stocks + Length_of_current_employment + Most_valuable_available_asset + Duration_of_Credit_monthly)

# summary(Logistic.2)

logit_P <- predict(Logistic.2, newdata = test, type = 'response')
logit_P <- ifelse(logit_P > 0.5, 1, 0)
actual <- as.numeric(unlist(test[,1]))
df <- data.frame(logit_P, actual)

CM2 <- confusionMatrix(as.factor(logit_P), as.factor(actual), mode = "everything", positive = "1")

# roc_score <- roc(actual, logit_P, plot = TRUE)

# plot(rocit(score = logit_P, class = actual))

rocplot <- ggplot(df, aes(m = logit_P, d = actual)) + 
  geom_roc(n.cuts = 20, labels = FALSE) + 
  geom_abline(linetype = "dashed")+
  theme_bw() + 
  labs(x = "1-Especificidad", y = "Sensibilidad", title = "Gráfico 2.8:  Curva ROC", caption = "Elaboración propia con datos extraídos de Kaggle")


```

\newpage

## Fichas de Resultados


\noindent\rule{14cm}{0.4pt}


- \textbf{Nombre Hallazgo:} Importancia de la re-categorización de la variable Purpose

- \textbf{Resumen en una oración:} En un estudio similar, la categorización de la variable es agrupada de manera diferente y se llega a una precisión del modelo menor a la que se llega de la forma propuesta

- \textbf{Principal característica:} La forma en la que se agrupen las diferentes categorías influye significativamente en el modelo

- \textbf{Problemas o posibles desafíos:} Para la segmentación se uso un criterio subjetivo por lo que si se probarán diferentes combinaciones la significancia de la variable puede aumentar o disminuir. 

- \textbf{Resumen en un parráfo:} En un estudio en el que se usa la base de datos estudiada, se propone una categorización que toma el valor de 1 si es para un auto nuevo, 2 para auto de segunda mano, 3 si es relacionado al hogar y 4 para cualquier otra razón. La categorización propuesta consistió en combinar las categorías 1 y 2 (referente a la compra de un auto) y dejar una categoría para todos aquellos créditos destinados al sector empresarial. Se tomó esa decisión debido a que tanto la categoría 1 y 2 comparten una misma naturaleza la cual es la compra de un auto. Como consecuencia, mediante la aplicación de las pruebas Chi-Cuadrado se llegó a que la variable Purpose no era significante por lo que se decidió descartar del modelo de regresión logística. Cabe resaltar que en el estudio consultado, sí toman en cuenta esta variable para la regresión y el resultado es una precisión menor a la encontrada sin usar la variable.



\noindent\rule{14cm}{0.4pt}



- \textbf{Nombre Hallazgo:} Pruebas de significancia sobre las variables usadas

- \textbf{Resumen en una oración:} La base presenta múltiples variables que se buscan reducir mediante la aplicación de pruebas con el fin de llegar a un modelo más optimizado.

- \textbf{Principal característica:}  La realización de la Prueba Chi-Cuadrado y la Prueba t arrojan resultados importantes a la hora de la selección de variables para la regresión. 

- \textbf{Problemas o posibles desafíos:} Las variables continuas no cumplen con los supuestos de normalidad para aplicar la Prueba t por lo que no se pudo verificar su significancia a la hora de determinar si un solicitante es buen deudor o no. 

- \textbf{Resumen en un parráfo:} La base cuenta con 20 diferentes variables predictoras. Se busca reducir esta cantidad de variables mediante la aplicación de pruebas como la Chi-Cuadrado y la Prueba t. Para la Prueba t se necesita un supuesto de normalidad en la distribución de los datos lo cual no cumplieron las variables de carácter continuo por lo que se descartó esta prueba para determinar la significancia de estas variables cuantitativas. Por otro lado, mediante la aplicación de la Prueba Chi-Cuadrado se descartaron un total de 10 variables que no parecían tener significancia con la elegibilidad de una persona solicitante. Entre las variables descartadas destacan si la persona tenía un teléfono o no (Telephone), el estado civil de la persona, si la persona era extranjera o no, la duración que lleva el solicitante viviendo en su ubicación actual, entre otras. 


\noindent\rule{14cm}{0.4pt}


- \textbf{Nombre Hallazgo:} Resultados de la Regresión Logística

- \textbf{Resumen en una oración:} Mediante la aplicación de la regresión logística no solo se logró el objetivo de clasificar al solicitante como buen deudor o no, sino que también se obtuvieron las variables más significantes que determinan a un solicitante como buen deudor. 

- \textbf{Principal característica:} Para la regresión se usaron las variables que fueron significativas en las pruebas hechas anteriormente. Inicialmente se obtuvieron resultados bastantes decentes. 

- \textbf{Problemas o posibles desafíos:}  Un desafío sería aumentar aún más la precisión del modelo. 

- \textbf{Resumen en un parráfo:} Se desarrollaron una regresión logística que arroja un área debajo de la curva de alrededor del 70 $\%$. La precisión es de un 75 $\%$  y la sensibilidad del 82 $\%$ aproximadamente. Asimismo, entre las variables más significantes de este modelo destaca el balance actual, el estado de pago actual del pasado crédito solicitado, la duración del crédito solicitado y el tiempo que lleva el solicitante en su actual empleo. Por otro lado, entre las variables que arrojan menos significancia destacan la edad del solicitante, el monto del crédito, el tipo de apartamento u hogar que tenga y si tiene créditos en otros bancos o instituciones. A partir de estos resultados se comenzará a usar modelos de cópulas para verificar la independecia existente entre la clasificación recién hecha y las variables significativas del modelo. 


\newpage

## Enlaces de Literatura

Para un primer acercamiento a contestar la pregunta de investigación, antes de poder determinar correlaciones entre las variables de estudio para determinar las pérdidas del banco por impago crediticio se necesita una manera de determinar alguna medida como el score crediticio por individuo. Es por esto que primero se va a realizar un modelo de clasificación de elegibilidad de crédito, para lo cual se implementó un modelo de regresión logística. Este tipo de regresión como menciona James, a diferencia de los métodos de regresión, se teoriza un nuevo tipo de regresión llamada regresión logística, la cual sirve para clasificar de manera binaria una variable. Entonces en vez de entrenar un modelo para determinar si hay una correlación entre la variable dependiente y las covariables, se entrena para clasificar en alguna de dos categorías a la variable dependiente de acuerdo a sus covariables.  Esta definición del modelo es similar a la que hace Chitarroni en su artículo ya que lo define como un instrumento de análisis multivariado y dependiendo del enfoque se puede utilizar para realizar predicciones o inferencia. Se menciona que es muy útil cuando la variable dependiente es de carácter dicotómico (binario). Asimismo, se aclara que cuando las covariables son categóricas estas deberían de recibir una transformación y convertirlas en variables "dummy", es decir, variables simuladas. 

Sin embargo, en el artículo expuesto por Chitarroni se le da más peso a las pruebas de significancia de variables así como a la interpretabilidad de los resultados enfocado más en un modelo predictivo de manera que especifica que este tipo de modelo se puede utilizar de manera predictiva, el cual no solo determina la probabilidad de la variable dependiente sino el peso que tienen las covariables para realizar la predicción, lo cual también ayuda a nivel interpretativo pues se pueden determinar que variables son más significativas que otras para el modelo. Mientras que en su libro James también  menciona diagnósticos de los modelos de clasificación que ayudan a determinar si un modelo es mejor que otro o está mejor ajustado como por ejemplo el concepto de especificidad y sensibilidad los cuales miden la tasa de falsos positivos y los falsos negativos respectivamente. Y como de acuerdo al tipo de problema para el cual se está realizando el modelo, se debe considerar ajustes del modelo para aumentar estas medidas. Otro diágnostico bastante utilizado es el de la curva ROC (Receiver Operation Curve), la cual sirve para graficar la tasa de falsos positivos con la sensibilidad del modelo. 

Tomando en consideración ambos enfoques se puede ver que con los resultados obtenidos en el modelo realizado se logra desarrollar una regresión logística que arroja un área debajo de la curva de alrededor del 70 $\%$. La precisión es de un 75 $\%$  y la sensibilidad del 82 $\%$ aproximadamente. Asimismo, entre las variables más significantes de este modelo destaca el balance actual, el estado de pago actual del pasado crédito solicitado, la duración del crédito solicitado y el tiempo que lleva el solicitante en su actual empleo. Por otro lado, entre las variables que arrojan menos significancia destacan la edad del solicitante, el monto del crédito, el tipo de apartamento u hogar que tenga y si tiene créditos en otros bancos o instituciones. A partir de estos resultados se comenzará a usar modelos de cópulas para verificar la independecia existente entre la clasificación recién hecha y las variables significativas del modelo.

Para la segunda parte del trabajo, donde ya se plantea el hecho de encontrar las pérdidas del banco, se planean utilizar modelos que involucren cópulas. Estos modelos sirven para encontrar distribuciones conjuntas que generalmente tienen un alto grado de correlación entre sí, por lo que analizarlas por separado no es lo más recomendable. Pues como menciona Escarela, las cópulas bivariadas son funciones que intentan correlacionar dos distribuciones univariadas por lo que estos modelos ayudan a obtener una distribución conjunta a partir de varias funciones de distribución asociadas a variables aleatorias con una cierta relación entre sí. Es decir, con este método se construye una distribución multivariada a partir de las distribuciones univariadas de las variables respectivas. Este tipo de modelo también resulta en una forma de estructurar la dependencia de estas parejas de variables aleatorias en distribuciones conjuntas. Es por esto que son tan populares puesto que tienen una gran flexibilidad para encontrar distribuciones conjuntas a partir de cualquier pareja aleatoria, lo que es usual tener en muchas disciplinas.

Sin embargo para el enfoque del trabajo estas no son de mucha utlidad pues como menciona Geidosch el  uso de cópulas convencionales presenta dos limitantes, la primera es que cuando el número de dimensiones es mayor que dos, el número de familias de cópulas aplicables se limita al caso elíptico y arquimediano. La segunda limitante es que solo dependencias simétricas y simples pueden ser modeladas y eso está lejos de ser considerado un escenario real. De ahí nace la necesidad de usar Vine Cópulas. Las Vines Cópulas se usan para describir cópulas de varias variables usando como base cópulas bivariadas o "pair-copulas" . La idea que existe de fondo con los Vines Cópulas es que distribuciones multivariadas se pueden descomponer secuencialmente en descomposiciones bivariadas y para llevar acabo dicho proceso se hace uso de los D-vine y los R-vine. Lo que se hace es tomar la densidad multivariada de la distribución y se transforma en funciones de densidad de las cópulas bivariadas. 

Estas limitantes son las que motivan también a Daul a encontrar modelos más generales ya que en su artículo extiende el concepto de t-Cópula para generar una nueva t-Cópula agrupada que describe de manera más efectiva la dependencia que existe entre factores de riesgo en el ámbito financiero. En el artículo también se explica el proceso de cálculo de los nuevos parámetros de las t-Cópulas agrupadas. Básicamente la idea que hay de fondo es juntar los diferentes factores de riesgo en esta t-Cópula agrupada donde las variables aleatorias dentro de esta cópula tienen una t-cópula con diferentes grados de libertad. Como resultado, esto da una estructura de dependencia más flexible que se ajusta mejor para modelos de riesgos que tengan múltiples factores. A modo de conclusión, utilizan el value-at-risk para comparar el modelo usando t-Cópulas con el modelo de t-Cópulas agrupadas y este segundo siempre arroja resultados con medidas de riesgo mayores, por lo que se tiene un modelo más conservador y preciso. 





